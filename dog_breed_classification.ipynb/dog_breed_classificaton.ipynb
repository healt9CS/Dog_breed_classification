import numpy as np
import pandas as pd
import seaborn as sns
import os
import gc
import matplotlib.pyplot as plt
import tensorflow as tf
from tqdm.autonotebook import tqdm
from keras import Sequential
from keras.callbacks import EarlyStopping
from keras.layers import Dense, Conv2D, Flatten
from keras.optimizers import Adam,SGD
from keras.callbacks import ReduceLROnPlateau
from keras.layers import Flatten, Dense, BatchNormalization, Activation, Dropout
from keras.layers import Lambda, Input,   GlobalAveragePooling2D
from keras.utils import to_categorical
from tensorflow.keras.models import Model
from keras.preprocessing.image import load_img
print("GPU", "avaliable(YESS!!!!))" if tf.config.list_physical_devices("GPU") else "not avaliable:(")
tf.config.list_physical_devices("GPU")
from google.colab import drive
drive.mount('/content/drive')
dataset_path = '/LENOVO/Desktop/dog/train'
labels = pd.read_csv('/content/labels.csv')
labels.head(10)
labels['breed'].value_counts()
labels.shape
print(labels)
labels.describe()
def barw(ax):

    for p in ax.patches:
      val = p.get_width()
      x = p.get_x()+ p.get_width()
      y = p.get_y() + p.get_height()/2
      ax.annotate(round(val,2),(x,y))

plt.figure(figsize = (15,30))
ax0 = sns.countplot(y=labels['breed'],order=labels['breed'].value_counts().index)
barw(ax0)
plt.show()
from IPython.display import display, Image
Image("/content/43572ba7edf772a95f539e57afd9eb43.jpg")
import os

directory_path = "/content/drive/MyDrive/My Drive/train"
if not os.path.exists(directory_path):
    os.makedirs(directory_path)
print("Files in the directory:", os.listdir(directory_path))
print("Length of labels['id']:", len(labels['id']))
if len(os.listdir(directory_path)) == len(labels['id']):
    print('Number of files matches the number of actual images!')
else:
    print('Number of files does not match the number of actual images!!')
print("Files in the directory:", os.listdir(directory_path))
classes = sorted(list(set(labels['breed'])))
n_classes = len(classes)
print('Total unique breed {}'.format(n_classes))
class_to_num = dict(zip(classes, range(n_classes)))
class_to_num
input_shape = (331,331,3)

def images_to_array(directory, label_dataframe, classes, target_size=input_shape):
    image_labels = label_dataframe['breed']
    images = np.zeros([len(label_dataframe), target_size[0], target_size[1], target_size[2]], dtype=np.uint8)
    y = np.zeros([len(label_dataframe), len(classes)], dtype=np.uint8)

    for ix, image_name in enumerate(tqdm(label_dataframe['id'].values)):
        img_dir = os.path.join(directory, image_name + '.jpg')
        img = load_img(img_dir, target_size=target_size)
        images[ix] = img
        del img
        dog_breed = image_labels[ix]
        y[ix, :] = to_categorical(class_to_num[dog_breed], num_classes=len(classes))

    return images, y

# Correct the function call
X, y = images_to_array(directory_path, labels, classes)
n = 25
plt.figure(figsize=(20, 20))

for i in range(n):
    ax = plt.subplot(5, 5, i+1)

    # Check if np.where(y[i] == 1) is not empty
    indices = np.where(y[i] == 1)[0]
    if len(indices) > 0:
        breed_index = indices[0]
        plt.title(classes[breed_index])
        plt.imshow(X[i].astype('int32'))
    else:
        plt.title("No breed found")
print(y)
print(classes)
lrr = ReduceLROnPlateau(monitor='val_acc', factor=.01, patience=3, min_lr=1e-5,verbose = 1)
EarlyStop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
batch_size= 32
epochs=50
learn_rate=.001
sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)
adam=Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=None,  amsgrad=False)
from keras.layers import Input, Lambda
from keras.models import Model
img_size = (331,331,3)

def get_features(model_name, model_preprocessor, input_size, data):
    input_layer = Input(input_size)
    preprocessor = Lambda(model_preprocessor)(input_layer)
    base_model = model_name(weights='imagenet', include_top=False, input_shape=input_size)(preprocessor)

    avg = GlobalAveragePooling2D()(base_model)
    feature_extractor = Model(inputs = input_layer, outputs = avg)
    feature_maps = feature_extractor.predict(data, verbose=1)
    print('Feature maps shape: ', feature_maps.shape)


    return feature_maps
from keras.applications.inception_v3 import InceptionV3, preprocess_input  # Correct the spelling

inception_preprocessor = preprocess_input  # Correct the spelling
inception_features = get_features(InceptionV3,
                                  inception_preprocessor,
                                  img_size, X)
del X
gc.collect()
final_features = np.concatenate([inception_features,

                                ], axis=-1)
print('Final feature maps shape', final_features.shape)
adam = Adam(learning_rate=0.001)  
lrr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)
model = Sequential()
model.add(Dropout(0.7,input_shape=(final_features.shape[1],)))
model.add(Dense(n_classes,activation= 'softmax'))
model.compile(optimizer=adam,
              loss='categorical_crossentropy',
              metrics=['accuracy'])
history = model.fit(final_features, y,
                    batch_size=batch_size,
                    epochs=epochs,
                    validation_split=0.2,
                    callbacks=[lrr,EarlyStop])
del inception_features
del final_features
gc.collect()
# Save the entire model (architecture and weights) to Google Drive
model.save('/content/drive/MyDrive/path_to_save_model.h5')

# Save only the weights to Google Drive
model.save_weights('/content/drive/MyDrive/path_to_save_weights.h5')
import os
from tqdm import tqdm
from keras.preprocessing.image import load_img
import numpy as np

def images_to_array_test(test_path, batch_size=32, img_size=(331, 331, 3)):
    test_filenames = [test_path + '/' + fname for fname in os.listdir(test_path)]
    data_size = len(test_filenames)
    images = np.zeros([data_size, img_size[0], img_size[1], 3], dtype=np.uint8)

    for start in range(0, data_size, batch_size):
        end = min(start + batch_size, data_size)
        batch_test_filenames = test_filenames[start:end]

        for ix, img_dir in enumerate(tqdm(batch_test_filenames)):
            img = load_img(img_dir, target_size=img_size)
            images[start + ix] = img

    print('Output Data Size: ', images.shape)
    return images

test_data = images_to_array_test('/content/drive/MyDrive/My Drive/test', batch_size=16, img_size=img_size)
def extract_features_array(model, preprocessor, img_size, data_array, batch_size=32):
    num_samples = len(data_array)
    features = np.zeros((num_samples, 2048), dtype=np.float32)

    for i in tqdm(range(0, num_samples, batch_size)):
        batch_images = data_array[i : i + batch_size]
        batch_features = get_features(model, preprocessor, img_size, batch_images)
        features[i : i + batch_size] = batch_features

    return features

test_features_array = extract_features_array(InceptionV3, inception_preprocessor, img_size, test_data, batch_size=16)
pred = model.predict(test_features_array)
print(pred[0])
print(f"Max value (probaility of prediction): {np.max(pred[0])}")
print(f"Sum: {np.sum(pred[0])}")
print(f"Max index: {np.argmax(pred[0])}")
print(f"Predicted label: {classes[np.argmax(pred[0])]}")
preds_df = pd.DataFrame(columns=["id"] + list(classes))
preds_df.head()
test_path = "/content/drive/MyDrive/My Drive/test"
preds_df["id"] = [os.path.splitext(path)[0] for path in os.listdir(test_path)]
preds_df.head()
preds_df.loc[:,list(classes)]= pred
preds_df.to_csv('submission.csv' ,index=None)
preds_df.head()
Image('/content/00a3edd22dc7859c487a64777fc8d093.jpg')
img_g = load_img('/content/00a3edd22dc7859c487a64777fc8d093.jpg' ,target_size = img_size)
img_g = np.expand_dims(img_g, axis=0)
img_g.shape
def get_features_single(model_name, model_preprocessor, input_size, data):
    input_layer = Input(input_size)
    preprocessor = Lambda(model_preprocessor)(input_layer)
    base_model = model_name(weights='imagenet', include_top=False, input_shape=input_size)(preprocessor)

    avg = GlobalAveragePooling2D()(base_model)
    feature_extractor = Model(inputs=input_layer, outputs=avg)
    feature_maps = feature_extractor.predict(data, verbose=1)
    print('Feature maps shape: ', feature_maps.shape)

    return feature_maps

def extract_features_single(model, preprocessor, img_size, data):
    features = get_features_single(model, preprocessor, img_size, data)
    return features
test_features = extract_features_single(InceptionV3, inception_preprocessor, img_size, img_g)
predg = model.predict(test_features)
print(f"Predicted label: {classes[np.argmax(predg[0])]}")
print(f"Probability of prediction: {round(np.max(predg[0]) * 100)} %")
